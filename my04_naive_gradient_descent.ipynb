{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 10/2/2020 Naive Gradient Descent\n",
    "#Bivariate linear regression, 2 parameters - Intercept and Slope\n",
    "\n",
    "#Make it work with fast.ai, so I understand how all the pieces connect together, and move on to MNIST digit classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#!pip install -Uqq fastbook\n",
    "#import fastbook\n",
    "#fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "#import graphviz\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Gradient Descent\n",
    "Bivariate, Two Parameters - Intercept and Slope\n",
    "\n",
    "Fit model to data.\n",
    "\n",
    "## 0. Create Data \n",
    "simplest dataset possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset 1: slope = 2, intercept = 0\n",
    "X_train = tensor([0, 1, 2]).float()\n",
    "y_train = tensor([0, 2, 4]).float()\n",
    "\n",
    "#dataset 2: slope = 1, intercept = 1\n",
    "X_train = tensor([1, 2, 3]).float()\n",
    "y_train = tensor([2, 3, 4]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot dataset\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title('Tiny dataset')\n",
    "plt.scatter(X_train, y_train, marker = 'o', c='b')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel ('y')\n",
    "plt.xticks(torch.arange(X_train.max()+2))\n",
    "plt.yticks(torch.arange(y_train.max()+2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#tensor rank\n",
    "print(len(X_train.shape), len(y_train.shape))\n",
    "#can also get rank with 'ndim'\n",
    "print(X_train.ndim, y_train.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch: Tensor Vocab  \n",
    "*rank* is the number of axes (or dimensions) in a tensor  \n",
    "*shape* is the size of each axis of a tensor.\n",
    "    - In this case, we can see that we have 6,131 images, each of size 28Ã—28 pixels  \n",
    "*length* of a tensor's shape is its rank. \n",
    "\n",
    "Using the term *dimension* may lead to confustion. When confused, it's helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Data\n",
    "none, tiny datasets are manually created per requirements\n",
    "\n",
    "## 2. Train Model with Gradient Descent\n",
    "`X_train` is input, represented as a vector  \n",
    "`params` are the weight(s) and bias, also a vector  \n",
    "We need some way to update the params to make them a little bit better. We can repeat that step a number of times, making the params better and better, until they are as good as we can make them.\n",
    "\n",
    "Find the best vector `params` which results in the best function to fit model to data. \n",
    "\n",
    "\n",
    "To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n",
    "\n",
    "1. *Initialize* the params.\n",
    "1. For each data point, use these params to *predict* data point value.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each param, how changing that params would change the loss\n",
    "1. *Step* (that is, change) all the params based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer).\n",
    "\n",
    "These seven steps, illustrated in [gradient_descent](###-summarizing-gradient-descent), are the key to the training of all deep learning models. It's amazing that this process can solve such complex problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PyTorch: special method `requires_grad_`? The magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for.  \n",
    "\n",
    "In deep learning, \"gradients\" usually means the _value_ of a function's derivative at a particular argument value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PyTorch: The \"backward\" here refers to *backpropagation*, which is the name given to the process of calculating the derivative of each layer. \n",
    "\n",
    "Later, when we calculate the gradients of a deep neural net from scratch. This is called the \"backward pass\" of the network, as opposed to the \"forward pass,\" which is where the activations are calculated. Life would probably be easier if `backward` was just called `calculate_grad`, but deep learning folks really do like to add jargon everywhere they can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 An End-to-End GD Example with a Tiny Dataset\n",
    "A dataset with 3 points is enough to understand Gradient Descent and see that it finds params pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global vars and functions\n",
    "#pick manually, trial and error\n",
    "#for dataset 1\n",
    "lr = .4\n",
    "#for dataset 2\n",
    "lr = .173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must tell PyTorch that we want gradients\n",
    "#def init_params(size, var=1.0): return (torch.randn(size)*var).requires_grad_()\n",
    "\n",
    "# hypothesis X*theta.T\n",
    "#not matrix multiplication yet, spell out linear fn\n",
    "def linear(input_X, params): \n",
    "    a,b = params\n",
    "    return input_X*a + b\n",
    "\n",
    "# cost function J with embedded loss function\n",
    "def mse(preds, targets): return ((preds-targets)**2).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually step thru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Initialize the parameters, hard coded here\n",
    "params = tensor([0., 0.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the predictions\n",
    "`y_hat = w*X+b` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: Calculate the predictions\n",
    "y_hat = linear(X_train, params)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 3: Calculate the loss\n",
    "loss = mse(y_hat, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 4: Calculate the gradients\n",
    "loss.backward()\n",
    "params.grad.shape,params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Step the params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: adjust step\n",
    "params.data -= lr * params.grad.data\n",
    "params.grad = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Repeat the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified to stop when good enough\n",
    "def apply_step(params, prn=True):\n",
    "    ## Step 2: Calculate the prediction\n",
    "    predictions_yhat = linear(X_train, params)\n",
    "    ## Step 3: Calculate the loss\n",
    "    loss = mse(predictions_yhat, y_train)\n",
    "    \n",
    "    if loss > 1e-6:\n",
    "        \n",
    "        ## Step 4: Calculate the gradients\n",
    "        loss.backward()\n",
    "        ## Step 5: adjust step    \n",
    "        params.data -= lr * params.grad.data\n",
    "        params.grad = None\n",
    "    else:\n",
    "        #print(\"good enough\")\n",
    "        return\n",
    "    \n",
    "    if prn: \n",
    "        print(\"loss: \", loss.item())\n",
    "        \n",
    "    return predictions_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  8.167820930480957\n",
      "tensor([3.3447, 5.6513, 7.9580], grad_fn=<AddBackward0>)\n",
      "loss:  6.901548862457275\n",
      "tensor([0.2912, 0.4617, 0.6323], grad_fn=<AddBackward0>)\n",
      "loss:  5.831747531890869\n",
      "tensor([3.1172, 5.2356, 7.3540], grad_fn=<AddBackward0>)\n",
      "loss:  4.927918910980225\n",
      "tensor([0.5387, 0.8521, 1.1654], grad_fn=<AddBackward0>)\n",
      "loss:  4.164299488067627\n",
      "tensor([2.9267, 4.8848, 6.8429], grad_fn=<AddBackward0>)\n",
      "loss:  3.5191328525543213\n",
      "tensor([0.7493, 1.1821, 1.6150], grad_fn=<AddBackward0>)\n",
      "loss:  2.974032163619995\n",
      "tensor([2.7671, 4.5887, 6.4103], grad_fn=<AddBackward0>)\n",
      "loss:  2.5134692192077637\n",
      "tensor([0.9285, 1.4612, 1.9939], grad_fn=<AddBackward0>)\n",
      "loss:  2.124324083328247\n",
      "tensor([2.6335, 4.3389, 6.0443], grad_fn=<AddBackward0>)\n",
      "loss:  1.7955141067504883\n",
      "tensor([1.0810, 1.6972, 2.3134], grad_fn=<AddBackward0>)\n",
      "loss:  1.517678141593933\n",
      "tensor([2.5219, 4.1281, 5.7344], grad_fn=<AddBackward0>)\n",
      "loss:  1.282907485961914\n",
      "tensor([1.2111, 1.8968, 2.5825], grad_fn=<AddBackward0>)\n",
      "loss:  1.0845205783843994\n",
      "tensor([2.4287, 3.9503, 5.4720], grad_fn=<AddBackward0>)\n",
      "loss:  0.9168737530708313\n",
      "tensor([1.3219, 2.0656, 2.8093], grad_fn=<AddBackward0>)\n",
      "loss:  0.7751979827880859\n",
      "tensor([2.3509, 3.8003, 5.2498], grad_fn=<AddBackward0>)\n",
      "loss:  0.6554663777351379\n",
      "tensor([1.4165, 2.2084, 3.0003], grad_fn=<AddBackward0>)\n",
      "loss:  0.5542750954627991\n",
      "tensor([2.2862, 3.6739, 5.0615], grad_fn=<AddBackward0>)\n",
      "loss:  0.4687494933605194\n",
      "tensor([1.4973, 2.3292, 3.1612], grad_fn=<AddBackward0>)\n",
      "loss:  0.39646074175834656\n",
      "tensor([2.2323, 3.5672, 4.9021], grad_fn=<AddBackward0>)\n",
      "loss:  0.3353569805622101\n",
      "tensor([1.5663, 2.4315, 3.2966], grad_fn=<AddBackward0>)\n",
      "loss:  0.283704549074173\n",
      "tensor([2.1876, 3.4772, 4.7669], grad_fn=<AddBackward0>)\n",
      "loss:  0.2400386780500412\n",
      "tensor([1.6254, 2.5180, 3.4106], grad_fn=<AddBackward0>)\n",
      "loss:  0.2031218558549881\n",
      "tensor([2.1505, 3.4014, 4.6524], grad_fn=<AddBackward0>)\n",
      "loss:  0.17190884053707123\n",
      "tensor([1.6759, 2.5912, 3.5065], grad_fn=<AddBackward0>)\n",
      "loss:  0.14551611244678497\n",
      "tensor([2.1198, 3.3375, 4.5552], grad_fn=<AddBackward0>)\n",
      "loss:  0.12319725751876831\n",
      "tensor([1.7192, 2.6532, 3.5871], grad_fn=<AddBackward0>)\n",
      "loss:  0.10432174056768417\n",
      "tensor([2.0945, 3.2837, 4.4728], grad_fn=<AddBackward0>)\n",
      "loss:  0.08835647255182266\n",
      "tensor([1.7564, 2.7057, 3.6549], grad_fn=<AddBackward0>)\n",
      "loss:  0.07485125213861465\n",
      "tensor([2.0737, 3.2383, 4.4029], grad_fn=<AddBackward0>)\n",
      "loss:  0.06342583894729614\n",
      "tensor([1.7883, 2.7501, 3.7119], grad_fn=<AddBackward0>)\n",
      "loss:  0.05375852808356285\n",
      "tensor([2.0566, 3.2001, 4.3436], grad_fn=<AddBackward0>)\n",
      "loss:  0.04557764157652855\n",
      "tensor([1.8158, 2.7877, 3.7597], grad_fn=<AddBackward0>)\n",
      "loss:  0.038653578609228134\n",
      "tensor([2.0426, 3.1679, 4.2932], grad_fn=<AddBackward0>)\n",
      "loss:  0.03279220685362816\n",
      "tensor([1.8394, 2.8196, 3.7999], grad_fn=<AddBackward0>)\n",
      "loss:  0.02782963030040264\n",
      "tensor([2.0312, 3.1408, 4.2504], grad_fn=<AddBackward0>)\n",
      "loss:  0.02362711727619171\n",
      "tensor([1.8598, 2.8467, 3.8336], grad_fn=<AddBackward0>)\n",
      "loss:  0.020067518576979637\n",
      "tensor([2.0219, 3.1180, 4.2140], grad_fn=<AddBackward0>)\n",
      "loss:  0.017051801085472107\n",
      "tensor([1.8773, 2.8696, 3.8618], grad_fn=<AddBackward0>)\n",
      "loss:  0.01449617464095354\n",
      "tensor([2.0145, 3.0988, 4.1831], grad_fn=<AddBackward0>)\n",
      "loss:  0.01232991274446249\n",
      "tensor([1.8925, 2.8890, 3.8855], grad_fn=<AddBackward0>)\n",
      "loss:  0.010493136942386627\n",
      "tensor([2.0086, 3.0827, 4.1568], grad_fn=<AddBackward0>)\n",
      "loss:  0.008935250341892242\n",
      "tensor([1.9057, 2.9055, 3.9053], grad_fn=<AddBackward0>)\n",
      "loss:  0.0076134526170790195\n",
      "tensor([2.0038, 3.0691, 4.1344], grad_fn=<AddBackward0>)\n",
      "loss:  0.0064915879629552364\n",
      "tensor([1.9171, 2.9195, 3.9219], grad_fn=<AddBackward0>)\n",
      "loss:  0.005539014935493469\n",
      "tensor([2.0001, 3.0577, 4.1153], grad_fn=<AddBackward0>)\n",
      "loss:  0.00472984230145812\n",
      "tensor([1.9269, 2.9313, 3.9357], grad_fn=<AddBackward0>)\n",
      "loss:  0.004042153246700764\n",
      "tensor([1.9972, 3.0481, 4.0990], grad_fn=<AddBackward0>)\n",
      "loss:  0.0034574754536151886\n",
      "tensor([1.9355, 2.9414, 3.9473], grad_fn=<AddBackward0>)\n",
      "loss:  0.002960090758278966\n",
      "tensor([1.9950, 3.0401, 4.0851], grad_fn=<AddBackward0>)\n",
      "loss:  0.002536717103794217\n",
      "tensor([1.9430, 2.9500, 3.9569], grad_fn=<AddBackward0>)\n",
      "loss:  0.002176135079935193\n",
      "tensor([1.9934, 3.0333, 4.0733], grad_fn=<AddBackward0>)\n",
      "loss:  0.00186882761772722\n",
      "tensor([1.9495, 2.9572, 3.9649], grad_fn=<AddBackward0>)\n",
      "loss:  0.0016067553078755736\n",
      "tensor([1.9922, 3.0277, 4.0632], grad_fn=<AddBackward0>)\n",
      "loss:  0.0013830872485414147\n",
      "tensor([1.9552, 2.9634, 3.9716], grad_fn=<AddBackward0>)\n",
      "loss:  0.001192049472592771\n",
      "tensor([1.9913, 3.0229, 4.0545], grad_fn=<AddBackward0>)\n",
      "loss:  0.001028736005537212\n",
      "tensor([1.9602, 2.9687, 3.9771], grad_fn=<AddBackward0>)\n",
      "loss:  0.0008890090975910425\n",
      "tensor([1.9908, 3.0190, 4.0471], grad_fn=<AddBackward0>)\n",
      "loss:  0.0007693544030189514\n",
      "tensor([1.9646, 2.9732, 3.9817], grad_fn=<AddBackward0>)\n",
      "loss:  0.0006667771376669407\n",
      "tensor([1.9905, 3.0157, 4.0408], grad_fn=<AddBackward0>)\n",
      "loss:  0.0005787514382973313\n",
      "tensor([1.9685, 2.9770, 3.9855], grad_fn=<AddBackward0>)\n",
      "loss:  0.0005031409091316164\n",
      "tensor([1.9904, 3.0129, 4.0354], grad_fn=<AddBackward0>)\n",
      "loss:  0.0004381093895062804\n",
      "tensor([1.9718, 2.9802, 3.9886], grad_fn=<AddBackward0>)\n",
      "loss:  0.00038211411447264254\n",
      "tensor([1.9904, 3.0106, 4.0307], grad_fn=<AddBackward0>)\n",
      "loss:  0.00033383199479430914\n",
      "tensor([1.9748, 2.9830, 3.9912], grad_fn=<AddBackward0>)\n",
      "loss:  0.00029214262031018734\n",
      "tensor([1.9906, 3.0086, 4.0267], grad_fn=<AddBackward0>)\n",
      "loss:  0.00025610430748201907\n",
      "tensor([1.9774, 2.9854, 3.9933], grad_fn=<AddBackward0>)\n",
      "loss:  0.00022490251285489649\n",
      "tensor([1.9908, 3.0070, 4.0233], grad_fn=<AddBackward0>)\n",
      "loss:  0.00019784427422564477\n",
      "tensor([1.9798, 2.9874, 3.9950], grad_fn=<AddBackward0>)\n",
      "loss:  0.0001743494503898546\n",
      "tensor([1.9911, 3.0057, 4.0203], grad_fn=<AddBackward0>)\n",
      "loss:  0.00015391102351713926\n",
      "tensor([1.9818, 2.9891, 3.9964], grad_fn=<AddBackward0>)\n",
      "loss:  0.0001361116737825796\n",
      "tensor([1.9915, 3.0046, 4.0177], grad_fn=<AddBackward0>)\n",
      "loss:  0.00012057617277605459\n",
      "tensor([1.9837, 2.9906, 3.9975], grad_fn=<AddBackward0>)\n",
      "loss:  0.00010699533595470712\n",
      "tensor([1.9918, 3.0037, 4.0155], grad_fn=<AddBackward0>)\n",
      "loss:  9.510819654678926e-05\n",
      "tensor([1.9853, 2.9919, 3.9984], grad_fn=<AddBackward0>)\n",
      "loss:  8.467560837743804e-05\n",
      "tensor([1.9922, 3.0029, 4.0136], grad_fn=<AddBackward0>)\n",
      "loss:  7.551431190222502e-05\n",
      "tensor([1.9867, 2.9929, 3.9991], grad_fn=<AddBackward0>)\n",
      "loss:  6.744753773091361e-05\n",
      "tensor([1.9926, 3.0023, 4.0120], grad_fn=<AddBackward0>)\n",
      "loss:  6.0338425100781024e-05\n",
      "tensor([1.9880, 2.9939, 3.9997], grad_fn=<AddBackward0>)\n",
      "loss:  5.40590554010123e-05\n",
      "tensor([1.9930, 3.0018, 4.0105], grad_fn=<AddBackward0>)\n",
      "loss:  4.850291588809341e-05\n",
      "tensor([1.9892, 2.9947, 4.0002], grad_fn=<AddBackward0>)\n",
      "loss:  4.35769725299906e-05\n",
      "tensor([1.9934, 3.0014, 4.0093], grad_fn=<AddBackward0>)\n",
      "loss:  3.92051697417628e-05\n",
      "tensor([1.9902, 2.9954, 4.0005], grad_fn=<AddBackward0>)\n",
      "loss:  3.5318505979375914e-05\n",
      "tensor([1.9938, 3.0010, 4.0082], grad_fn=<AddBackward0>)\n",
      "loss:  3.185415698681027e-05\n",
      "tensor([1.9911, 2.9960, 4.0008], grad_fn=<AddBackward0>)\n",
      "loss:  2.8763708542101085e-05\n",
      "tensor([1.9942, 3.0007, 4.0072], grad_fn=<AddBackward0>)\n",
      "loss:  2.6002775484812446e-05\n",
      "tensor([1.9920, 2.9965, 4.0010], grad_fn=<AddBackward0>)\n",
      "loss:  2.3532542400062084e-05\n",
      "tensor([1.9946, 3.0005, 4.0064], grad_fn=<AddBackward0>)\n",
      "loss:  2.1316400307114236e-05\n",
      "tensor([1.9927, 2.9969, 4.0011], grad_fn=<AddBackward0>)\n",
      "loss:  1.9326886103954166e-05\n",
      "tensor([1.9950, 3.0003, 4.0057], grad_fn=<AddBackward0>)\n",
      "loss:  1.753970173012931e-05\n",
      "tensor([1.9934, 2.9973, 4.0012], grad_fn=<AddBackward0>)\n",
      "loss:  1.5930569134070538e-05\n",
      "tensor([1.9953, 3.0002, 4.0051], grad_fn=<AddBackward0>)\n",
      "loss:  1.4480905520031229e-05\n",
      "tensor([1.9940, 2.9976, 4.0012], grad_fn=<AddBackward0>)\n",
      "loss:  1.3173784282116685e-05\n",
      "tensor([1.9956, 3.0001, 4.0045], grad_fn=<AddBackward0>)\n",
      "loss:  1.1992639883828815e-05\n",
      "tensor([1.9945, 2.9979, 4.0013], grad_fn=<AddBackward0>)\n",
      "loss:  1.092288857762469e-05\n",
      "tensor([1.9959, 3.0000, 4.0040], grad_fn=<AddBackward0>)\n",
      "loss:  9.956459507520776e-06\n",
      "tensor([1.9950, 2.9982, 4.0013], grad_fn=<AddBackward0>)\n",
      "loss:  9.07951016415609e-06\n",
      "tensor([1.9962, 2.9999, 4.0036], grad_fn=<AddBackward0>)\n",
      "loss:  8.285450348921586e-06\n",
      "tensor([1.9955, 2.9984, 4.0013], grad_fn=<AddBackward0>)\n",
      "loss:  7.563269264210248e-06\n",
      "tensor([1.9965, 2.9999, 4.0032], grad_fn=<AddBackward0>)\n",
      "loss:  6.908071100042434e-06\n",
      "tensor([1.9959, 2.9986, 4.0012], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_loop = 100\n",
    "for i in range(n_loop): \n",
    "    p = apply_step(params) \n",
    "    if not p == None: print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0031, 0.9937], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(params, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: stop\n",
    "THe model learns the best parameters and returns them in the `params` vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"591pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 591.49 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-74 587.49,-74 587.49,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"126.1\" cy=\"-18\" rx=\"35.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"126.1\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.02,-18C62.26,-18 71.62,-18 80.78,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.96,-21.5 90.96,-18 80.96,-14.5 80.96,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"225.19\" cy=\"-52\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.19\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.44,-27.93C166.61,-31.84 179.52,-36.36 191.11,-40.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"190.29,-43.84 200.88,-43.84 192.6,-37.23 190.29,-43.84\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"361.84\" cy=\"-52\" rx=\"39.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"361.84\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.47,-52C269.35,-52 291.8,-52 311.88,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.13,-55.5 322.13,-52 312.13,-48.5 312.13,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"465.49\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"465.49\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.17,-41.52C405.92,-37.59 419.32,-33.11 431.25,-29.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.48,-32.4 440.85,-25.91 430.26,-25.76 432.48,-32.4\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M438.29,-18C380.72,-18 242.6,-18 171.32,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.3,-14.5 161.3,-18 171.3,-21.5 171.3,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.19\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"556.49\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.49\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.71,-18C501.04,-18 510.4,-18 519.3,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"519.45,-21.5 529.45,-18 519.45,-14.5 519.45,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f79e7b87c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Putting It All Together"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m56"
  },
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

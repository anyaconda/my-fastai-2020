{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 9/30/2020 My Own MNIST.  3 or 7 classifier\n",
    "#Start with manual Gradient Descent\n",
    "#Multivariate logistic regression, parameters - weights & bias\n",
    "\n",
    "#Make it work with fast.ai, so I understand how all the pieces connect together, and move on to MNIST digit classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#!pip install -Uqq fastbook\n",
    "#import fastbook\n",
    "#fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "#import graphviz\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Digit Classifier\n",
    "Create a model that can recognize 3s and 7s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Data \n",
    "From pickle  \n",
    "Common layout for ML datasets: separate folders for train and validation (and/or) test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pwd\n",
    "\n",
    "X_train = torch.load( 'myData/04_mnist_sample_X_train.pkl') #class torch.Tensor\n",
    "y_train = torch.load( 'myData/04_mnist_sample_y_train.pkl') #class torch.Tensor\n",
    "X_valid = torch.load( 'myData/04_mnist_sample_X_valid.pkl') \n",
    "y_valid = torch.load( 'myData/04_mnist_sample_y_valid.pkl')\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "#tensor rank\n",
    "print(len(X_train.shape), len(y_train.shape))\n",
    "#can also get rank with 'ndim'\n",
    "#print(X_train.ndim, y_train.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> python PIL: `Image` class from the Python Imaging Library (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically.\n",
    "\n",
    "> pandas: background_gradient() color-code the values using a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loak at one image\n",
    "#im3_path = threes[1]\n",
    "#im3 = Image.open(im3_path)\n",
    "#print(\"Type: \", type(im3))\n",
    "#im3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert an image PyTorch tensor\n",
    "##preview data\n",
    "#tensor(im3)[4:10,4:10]\n",
    "\n",
    "##shows us clearly how the image is created from the pixel values\n",
    "#im3_t = tensor(im3)\n",
    "#df = pd.DataFrame(im3_t[4:15,4:22])\n",
    "#df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how an image looks like to a computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch: Tensor Vocab  \n",
    "*rank* is the number of axes (or dimensions) in a tensor  \n",
    "*shape* is the size of each axis of a tensor.\n",
    "    - In this case, we can see that we have 6,131 images, each of size 28×28 pixels  \n",
    "*length* of a tensor's shape is its rank. \n",
    "\n",
    "Using the term *dimension* may lead to confustion. When confused, it's helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Model with Gradient Descent\n",
    "`X` is the image, represented as a vector  \n",
    "`w` is the weights, also a vector  \n",
    "We need some way to update the weights to make them a little bit better. We can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.\n",
    "\n",
    "Find the best vector `w` which results in the best function for recognising 3s, which means the result to be high for those images that are actually 3s, and low for those images that are not. \n",
    "\n",
    "\n",
    "To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each image, use these weights to *predict* whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer).\n",
    "\n",
    "These seven steps, illustrated in [gradient_descent](###-summarizing-gradient-descent), are the key to the training of all deep learning models. That deep learning turns out to rely entirely on these steps is extremely surprising and counterintuitive. It's amazing that this process can solve such complex problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the general approach to each one generally follows some basic principles. Here are a few guidelines:\n",
    "\n",
    "- Initialize:: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\n",
    "- Loss:: This is what Samuel referred to when he spoke of *testing the effectiveness of any current weight assignment in terms of actual performance*. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\n",
    "- Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating *gradients*. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\n",
    "- Stop:: Once we've decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PyTorch: special method `requires_grad_`? The magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for.  \n",
    "\n",
    "In deep learning, \"gradients\" usually means the _value_ of a function's derivative at a particular argument value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PyTorch: The \"backward\" here refers to *backpropagation*, which is the name given to the process of calculating the derivative of each layer. \n",
    "\n",
    "Later, when we calculate the gradients of a deep neural net from scratch. This is called the \"backward pass\" of the network, as opposed to the \"forward pass,\" which is where the activations are calculated. Life would probably be easier if `backward` was just called `calculate_grad`, but deep learning folks really do like to add jargon everywhere they can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 GD Manually with MNIST tiny subset\n",
    "A dataset with more dimensions to understand Gradient Descent and see that it finds params pretty well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 784]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#limit X #$actemp\n",
    "X_train_min = X_train[:3]\n",
    "y_train_min = y_train[:3]\n",
    "    \n",
    "X_train_min.shape, y_train_min.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproducibility $accheck\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "#Global vars and functions\n",
    "lr = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GD functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must tell PyTorch that we want gradients\n",
    "def init_params(size, var=1.0): return (torch.randn(size)*var).requires_grad_()\n",
    "\n",
    "# hypothesis X*theta.T\n",
    "def linear(input_X): return input_X@weights + bias\n",
    "\n",
    "# cost function J with embedded loss function\n",
    "def mnist_loss(prediction_yhat, output_y):\n",
    "    #scale predicitons\n",
    "    prediction_yhat = prediction_yhat.sigmoid()\n",
    "    return torch.where(output_y==1, 1-prediction_yhat, prediction_yhat).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually step thru\n",
    "#### Step 1: Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Initialize the parameters\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269],\n",
      "        [ 1.4873],\n",
      "        [ 0.9007],\n",
      "        [-2.1055],\n",
      "        [ 0.6784],\n",
      "        [-1.2345],\n",
      "        [-0.0431],\n",
      "        [-1.6047],\n",
      "        [-0.7521],\n",
      "        [ 1.6487]], grad_fn=<SliceBackward>)\n",
      "tensor([0.3472], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "original_weights = weights.clone()\n",
    "original_bias = bias.clone()\n",
    "print(original_weights[:10], original_bias, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the predictions\n",
    "`y_hat = w*X+b`  \n",
    "really `sigmoid(y_hat = w*X+b)`  \n",
    "sigmoid implemented in the `mnist_loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: Calculate the predictions\n",
    "y_hat = linear(X_train_min)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9993, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 3: Calculate the loss\n",
    "loss = mnist_loss(y_hat, y_train_min)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-9.4232e-05), tensor([-0.0007]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 4: Calculate the gradients\n",
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Step the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: adjust step\n",
    "weights.data -= lr * weights.grad.data\n",
    "weights.grad = None\n",
    "bias.data -= lr * bias.grad.data\n",
    "bias.grad = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Repeat the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_step(weights, bias, prn=True):\n",
    "    ## Step 2: Calculate the prediction\n",
    "    predictions_yhat = linear(X_train_min)\n",
    "    ## Step 3: Calculate the loss\n",
    "    loss = mnist_loss(predictions_yhat, y_train_min)\n",
    "    ## Step 4: Calculate the gradients\n",
    "    loss.backward()\n",
    "    ## Step 5: adjust step    \n",
    "    weights.data -= lr * weights.grad.data\n",
    "    weights.grad = None\n",
    "    bias.data -= lr * bias.grad.data\n",
    "    bias.grad = None\n",
    "    \n",
    "    if prn: \n",
    "        print(\"loss: \", loss.item())\n",
    "        \n",
    "    return predictions_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.9992944598197937\n",
      "predictions: \n",
      " tensor([[ -6.1677],\n",
      "        [-10.5913],\n",
      "        [-20.8409]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9992437362670898\n",
      "predictions: \n",
      " tensor([[ -6.0980],\n",
      "        [-10.5407],\n",
      "        [-20.7924]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9991855025291443\n",
      "predictions: \n",
      " tensor([[ -6.0233],\n",
      "        [-10.4864],\n",
      "        [-20.7403]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9991176128387451\n",
      "predictions: \n",
      " tensor([[ -5.9429],\n",
      "        [-10.4280],\n",
      "        [-20.6843]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9990377426147461\n",
      "predictions: \n",
      " tensor([[ -5.8558],\n",
      "        [-10.3647],\n",
      "        [-20.6235]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9989424347877502\n",
      "predictions: \n",
      " tensor([[ -5.7608],\n",
      "        [-10.2957],\n",
      "        [-20.5574]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9988269209861755\n",
      "predictions: \n",
      " tensor([[ -5.6564],\n",
      "        [-10.2199],\n",
      "        [-20.4846]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9986839294433594\n",
      "predictions: \n",
      " tensor([[ -5.5406],\n",
      "        [-10.1359],\n",
      "        [-20.4040]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9985027313232422\n",
      "predictions: \n",
      " tensor([[ -5.4108],\n",
      "        [-10.0417],\n",
      "        [-20.3135]], grad_fn=<AddBackward0>)\n",
      "loss:  0.9982664585113525\n",
      "predictions: \n",
      " tensor([[ -5.2632],\n",
      "        [ -9.9346],\n",
      "        [-20.2107]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_loop = 10\n",
    "for i in range(n_loop): \n",
    "    p = apply_step(weights, bias) \n",
    "    if not p == None: print(\"predictions: \\n\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9269],\n",
      "        [ 1.4873],\n",
      "        [ 0.9007],\n",
      "        [-2.1055],\n",
      "        [ 0.6784],\n",
      "        [-1.2345],\n",
      "        [-0.0431],\n",
      "        [-1.6047],\n",
      "        [-0.7521],\n",
      "        [ 1.6487]], grad_fn=<SliceBackward>)\n",
      "tensor([0.3587], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weights[:10],bias, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: stop\n",
    "THe model is on the way to learn the best parameters and returns them in the `weights` vector + `bias`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"591pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 591.49 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-74 587.49,-74 587.49,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"126.1\" cy=\"-18\" rx=\"35.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"126.1\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.02,-18C62.26,-18 71.62,-18 80.78,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.96,-21.5 90.96,-18 80.96,-14.5 80.96,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"225.19\" cy=\"-52\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"225.19\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.44,-27.93C166.61,-31.84 179.52,-36.36 191.11,-40.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"190.29,-43.84 200.88,-43.84 192.6,-37.23 190.29,-43.84\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"361.84\" cy=\"-52\" rx=\"39.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"361.84\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.47,-52C269.35,-52 291.8,-52 311.88,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.13,-55.5 322.13,-52 312.13,-48.5 312.13,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"465.49\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"465.49\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.17,-41.52C405.92,-37.59 419.32,-33.11 431.25,-29.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"432.48,-32.4 440.85,-25.91 430.26,-25.76 432.48,-32.4\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M438.29,-18C380.72,-18 242.6,-18 171.32,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.3,-14.5 161.3,-18 171.3,-21.5 171.3,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.19\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"556.49\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.49\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.71,-18C501.04,-18 510.4,-18 519.3,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"519.45,-21.5 529.45,-18 519.45,-14.5 519.45,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f05ced06250>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Putting It All Together: GD More Automated with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GD functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original code, with original var names\n",
    "def linear1(xb): return xb@weights + bias\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    #scale predicitons\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
    "\n",
    "## Step 2-4: Calculate the gradients\n",
    "def calc_grad(xb, yb, model):\n",
    "    # Step 2: Calculate the predictions\n",
    "    preds = model(xb)\n",
    "    # Step 3: Calculate the loss\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    # Step 4: Calculate the gradients\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to train model and check performance metric - in one run, not in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_run(model, lr, params):\n",
    "    for X,y in zip(X_train, y_train):\n",
    "        calc_grad(X, y, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "\n",
    "#model accuracy\n",
    "def run_accuracy(X, y):\n",
    "    preds = X.sigmoid()\n",
    "    correct = (preds>0.5) == y\n",
    "    return correct.float().mean()\n",
    "\n",
    "#validate model\n",
    "def validate_run(model):\n",
    "    accs = [run_accuracy(model(X), y) for X,y in zip(X_valid, y_valid)]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More automated step thru\n",
    "#### Step 1: Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1268],\n",
      "        [-2.4521],\n",
      "        [ 0.4160],\n",
      "        [ 1.9025],\n",
      "        [-1.3525],\n",
      "        [-0.1303],\n",
      "        [ 1.7551],\n",
      "        [ 0.0675],\n",
      "        [ 0.7402],\n",
      "        [ 1.4162]], grad_fn=<SliceBackward>)\n",
      "tensor([-0.2762], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Initialize the parameters\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "\n",
    "print(weights[:10], bias, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5049"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first GD run\n",
    "params = weights,bias\n",
    "train_run(linear1, lr, params)\n",
    "validate_run(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute a run: steps 2 thru 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6472 0.7699 0.8189 0.8538 0.8739 0.894 0.9082 0.9171 0.9249 0.9274 0.9308 0.9333 0.9328 0.9357 0.9377 0.9411 0.9426 0.9446 0.9465 0.9465 "
     ]
    }
   ],
   "source": [
    "# 20 more runs\n",
    "#reset lr, lr=1. was diverging too fast for the entire dataset\n",
    "lr = 0.25\n",
    "params = weights,bias\n",
    "\n",
    "for i in range(20):\n",
    "    train_run(linear1, lr, params)\n",
    "    print(validate_run(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1268],\n",
      "        [-2.4521],\n",
      "        [ 0.4160],\n",
      "        [ 1.9025],\n",
      "        [-1.3525],\n",
      "        [-0.1303],\n",
      "        [ 1.7551],\n",
      "        [ 0.0675],\n",
      "        [ 0.7402],\n",
      "        [ 1.4162]], grad_fn=<SliceBackward>)\n",
      "tensor([-2.6875], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor([0.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(weights[:10],bias, sep='\\n')\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done\n",
    "\n",
    "### 2.3 Putting It All Together: Stochastic GD with MNIST\n",
    "run batches of data\n",
    "\n",
    "#### 2.3.0 Load data\n",
    "from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pwd\n",
    "\n",
    "dset = torch.load( 'myData/04_mnist_sample_dset.pkl') #class list of tuples of torch.Tensor (X, y)\n",
    "valid_dset = torch.load( 'myData/04_mnist_sample_dset_valid.pkl') #class list of tuples of torch.Tensor (X_valid, y_valid)\n",
    "\n",
    "dset.__class__, valid_dset.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([784]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(dset[0].__class__)\n",
    "print(dset[0][0].__class__, dset[0][1].__class__)\n",
    "print(dset[0][0].shape, dset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make batches\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define SGD functions to train model and check performance metric - in batches\n",
    "Everything just like above, but now split the dataset into batches and have a for loop for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD step thru\n",
    "#### Step 1: Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2367],\n",
      "        [-0.8144],\n",
      "        [-0.7985],\n",
      "        [ 0.5182],\n",
      "        [-1.1608],\n",
      "        [-1.0227],\n",
      "        [-0.5064],\n",
      "        [ 0.7802],\n",
      "        [ 0.6285],\n",
      "        [-0.7066]], grad_fn=<SliceBackward>)\n",
      "tensor([-0.6796], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Initialize the parameters\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "\n",
    "print(weights[:10], bias, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6546"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##first SGD run\n",
    "lr = 1.\n",
    "params = weights,bias\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157 0.8914 0.929 0.9412 0.9485 0.9529 0.9559 0.9573 0.9588 0.9598 0.9612 0.9642 0.9647 0.9661 0.9652 0.9661 0.9691 0.9686 0.9701 0.9716 "
     ]
    }
   ],
   "source": [
    "#run 20 more\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Creating an Optimizer \n",
    "with PyTorch module `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4213"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8667 0.8198 0.9102 0.9326 0.9468 0.9555 0.9614 0.9653 0.9678 0.9697 0.9717 0.9731 0.9751 0.9761 0.9765 0.9775 0.9775 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8457 0.833 0.9111 0.9326 0.9463 0.9546 0.9624 0.9653 0.9673 0.9687 0.9712 0.9731 0.9746 0.9761 0.977 0.9775 0.978 0.9785 0.979 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636950</td>\n",
       "      <td>0.503386</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480388</td>\n",
       "      <td>0.203609</td>\n",
       "      <td>0.827282</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.177803</td>\n",
       "      <td>0.173354</td>\n",
       "      <td>0.844455</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.078724</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.912659</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042229</td>\n",
       "      <td>0.076654</td>\n",
       "      <td>0.932777</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027967</td>\n",
       "      <td>0.061644</td>\n",
       "      <td>0.948479</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.052264</td>\n",
       "      <td>0.955348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.046005</td>\n",
       "      <td>0.962218</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018124</td>\n",
       "      <td>0.041601</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.038352</td>\n",
       "      <td>0.967125</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jargon Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "1. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
    "1. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "1. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "1. What is a \"rank-3 tensor\"?\n",
    "1. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "1. What are RMSE and L1 norm?\n",
    "1. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "1. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "1. What is broadcasting?\n",
    "1. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "1. What is SGD?\n",
    "1. Why does SGD use mini-batches?\n",
    "1. What are the seven steps in SGD for machine learning?\n",
    "1. How do we initialize the weights in a model?\n",
    "1. What is \"loss\"?\n",
    "1. Why can't we always use a high learning rate?\n",
    "1. What is a \"gradient\"?\n",
    "1. Do you need to know how to calculate gradients yourself?\n",
    "1. Why can't we use accuracy as a loss function?\n",
    "1. Draw the sigmoid function. What is special about its shape?\n",
    "1. What is the difference between a loss function and a metric?\n",
    "1. What is the function to calculate new weights using a learning rate?\n",
    "1. What does the `DataLoader` class do?\n",
    "1. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "1. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
    "1. What does `view` do in PyTorch?\n",
    "1. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "1. What does the `@` operator do in Python?\n",
    "1. What does the `backward` method do?\n",
    "1. Why do we have to zero the gradients?\n",
    "1. What information do we have to pass to `Learner`?\n",
    "1. Show Python or pseudocode for the basic steps of a training loop.\n",
    "1. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
    "1. What is an \"activation function\"?\n",
    "1. What's the difference between `F.relu` and `nn.ReLU`?\n",
    "1. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your own implementation of `Learner` from scratch, based on the training loop shown in this chapter.\n",
    "1. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$xtra myTry ok to delete\n",
    "\n",
    "## function MSE\n",
    "##def mse(preds, targets): return ((preds-targets)**2).mean()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m56"
  },
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

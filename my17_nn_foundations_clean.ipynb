{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqq fastbook","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import fastbook\nfastbook.setup_book()","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#hide $error ModuleNotFoundError: No module named 'fastai.gen_doc'\n#from fastai.gen_doc.nbdoc import *","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# A Neural Net from the Foundations","metadata":{}},{"cell_type":"markdown","source":"## Building a Neural Net Layer from Scratch","metadata":{}},{"cell_type":"markdown","source":"### Modeling a Neuron","metadata":{}},{"cell_type":"markdown","source":"### Matrix Multiplication from Scratch\nLet's write a function that computes the matrix product of two tensors, before we allow ourselves to use the PyTorch version of it. We will only use the indexing in PyTorch tensors:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import tensor","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We'll need three nested `for` loops:  \n- i for the row indices  \n- j for the column indices, and  \n- k for the inner sum (ac==br)","metadata":{}},{"cell_type":"code","source":"#define function\n#dstruct: use indexing in torch tensor\n#need 3 nested loops: i for row indices, j for col indices, k for the inner sum (ac==br)\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#define tensors\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784,10)\n\nm1.shape, m2.shape","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(torch.Size([5, 784]), torch.Size([784, 10]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's time our function, using the Jupyter \"magic\" command `%time`:","metadata":{}},{"cell_type":"code","source":"%time t1=matmul(m1, m2)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 1.01 s, sys: 2.29 ms, total: 1.01 s\nWall time: 1.01 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"And see how that compares to PyTorch's built-in `@`:","metadata":{}},{"cell_type":"code","source":"%timeit -n 20 t2=m1@m2","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The slowest run took 30.06 times longer than the fastest. This could mean that an intermediate result is being cached.\n38 µs ± 74.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see, in Python three nested loops is a very bad idea! Python is a slow language, and this isn't going to be very efficient. We see here that PyTorch is around 100,000 times faster than Python—and that's before we even start using the GPU!\n\nWhere does this difference come from? PyTorch didn't write its matrix multiplication in Python, but rather in C++ to make it fast. In general, whenever we do computations on tensors we will need to *vectorize* them so that we can take advantage of the speed of PyTorch, usually by using two techniques: elementwise arithmetic and broadcasting.","metadata":{}},{"cell_type":"code","source":"#my MatMul w/torch\n#src https://pytorch.org/docs/stable/generated/torch.matmul.html\n\nprint(torch.matmul(m1,m2).size())\nprint(torch.matmul(m1,m2))\n%time torch_matmul=torch.matmul(m1,m2)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([5, 10])\ntensor([[ 24.4247, -14.3996,  38.5341,  77.2113, -10.4177,  26.7255,  20.0069, -20.6713,   4.6560, -30.0900],\n        [ -1.5383, -25.5927, -17.6726,  33.6043,   2.1063,  -7.6736, -25.8539, -85.6422,  11.2662,  53.2808],\n        [ 44.6554, -39.9383, -26.3721,  28.6456, -15.0637,   7.8983, -16.4531, -17.2903, -36.5638,   7.7106],\n        [ 10.8838, -31.5166,  28.7179,  33.3447,  13.2409, -40.4614, -50.5393, -25.0790,   4.7932,  -5.1468],\n        [  3.0999,  18.2041,  34.2513, -46.1954, -19.2378, -43.5744,  -5.8470,  -9.7701, -26.3726, -30.8362]])\nCPU times: user 0 ns, sys: 476 µs, total: 476 µs\nWall time: 417 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"m1@m2","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[ 24.4247, -14.3996,  38.5341,  77.2113, -10.4177,  26.7255,  20.0069, -20.6713,   4.6560, -30.0900],\n        [ -1.5383, -25.5927, -17.6726,  33.6043,   2.1063,  -7.6736, -25.8539, -85.6422,  11.2662,  53.2808],\n        [ 44.6554, -39.9383, -26.3721,  28.6456, -15.0637,   7.8983, -16.4531, -17.2903, -36.5638,   7.7106],\n        [ 10.8838, -31.5166,  28.7179,  33.3447,  13.2409, -40.4614, -50.5393, -25.0790,   4.7932,  -5.1468],\n        [  3.0999,  18.2041,  34.2513, -46.1954, -19.2378, -43.5744,  -5.8470,  -9.7701, -26.3726, -30.8362]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Elementwise Arithmetic","metadata":{}},{"cell_type":"code","source":"a = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na + b","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([12., 14.,  3.])"},"metadata":{}}]},{"cell_type":"code","source":"a < b","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([False,  True,  True])"},"metadata":{}}]},{"cell_type":"code","source":"(a < b).all(), (a==b).all()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(tensor(False), tensor(False))"},"metadata":{}}]},{"cell_type":"code","source":"(a + b).mean().item()","metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"9.666666984558105"},"metadata":{}}]},{"cell_type":"code","source":"m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm*m","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])"},"metadata":{}}]},{"cell_type":"code","source":"n = tensor([[1., 2, 3], [4,5,6]])\nm*n","metadata":{"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-add73c4f74e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"],"ename":"RuntimeError","evalue":"The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0","output_type":"error"}]},{"cell_type":"markdown","source":"With elementwise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the `i`-th row of `a` and the `j`-th column of `b` before summing all the elements, which will speed things up because the inner loop will now be executed by PyTorch at C speed. \n\nTo access one column or row, we can simply write `a[i,:]` (can be abbreviated to `a[i]`) or `b[:,j]`. With all of that in mind, we can write a new version of our matrix multiplication:","metadata":{}},{"cell_type":"code","source":"def matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum()\n    return c","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%timeit -n 20 t3 = matmul(m1,m2)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1.53 ms ± 96.5 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Broadcasting","metadata":{}},{"cell_type":"markdown","source":"#### Broadcasting with a scalar","metadata":{}},{"cell_type":"code","source":"a = tensor([10., 6, -4])\na > 0","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([ True,  True, False])"},"metadata":{}}]},{"cell_type":"code","source":"m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n(m - 5) / 2.73","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor([[-1.4652, -1.0989, -0.7326],\n        [-0.3663,  0.0000,  0.3663],\n        [ 0.7326,  1.0989,  1.4652]])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Broadcasting a vector to a matrix","metadata":{}},{"cell_type":"code","source":"c = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nm.shape,c.shape","metadata":{"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3, 3]), torch.Size([3]))"},"metadata":{}}]},{"cell_type":"code","source":"m + c","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])"},"metadata":{}}]},{"cell_type":"code","source":"c.expand_as(m)","metadata":{"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])"},"metadata":{}}]},{"cell_type":"code","source":"t = c.expand_as(m)\nt.storage()","metadata":{"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":" 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]"},"metadata":{}}]},{"cell_type":"code","source":"t.stride(), t.shape","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"((0, 1), torch.Size([3, 3]))"},"metadata":{}}]},{"cell_type":"code","source":"c + m","metadata":{"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])"},"metadata":{}}]},{"cell_type":"code","source":"c = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m","metadata":{"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([[11., 22., 33.],\n        [14., 25., 36.]])"},"metadata":{}}]},{"cell_type":"code","source":"c = tensor([10.,20])\nm = tensor([[1., 2, 3], [4,5,6]])\nc+m","metadata":{"trusted":true},"execution_count":28,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-64bbbad4d99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1","output_type":"error"}]},{"cell_type":"code","source":"c = tensor([10.,20,30])\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\nc = c.unsqueeze(1)\nm.shape,c.shape","metadata":{"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3, 3]), torch.Size([3, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"c+m","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])"},"metadata":{}}]},{"cell_type":"code","source":"t = c.expand_as(m)\nt.storage()","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":" 10.0\n 20.0\n 30.0\n[torch.FloatStorage of size 3]"},"metadata":{}}]},{"cell_type":"code","source":"t.stride(), t.shape","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"((1, 0), torch.Size([3, 3]))"},"metadata":{}}]},{"cell_type":"code","source":"c = tensor([10.,20,30])\nc.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape","metadata":{"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"c.shape, c[None,:].shape,c[:,None].shape","metadata":{"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"c[None].shape,c[...,None].shape","metadata":{"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 3]), torch.Size([3, 1]))"},"metadata":{}}]},{"cell_type":"code","source":"def matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        c[i] = (a[i] * b.T).sum(1)\n    return c","metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n#       c[i] = (a[i] * b.T).sum(1) # my\n        c[i]   = (a[i].unsqueeze(-1) * b).sum(dim=0)\n    return c","metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"%timeit -n 20 t4 = matmul(m1,m2)","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"268 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n","output_type":"stream"}]},{"cell_type":"code","source":"mystop","metadata":{"trusted":true},"execution_count":39,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-255d8eb4da7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmystop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'mystop' is not defined"],"ename":"NameError","evalue":"name 'mystop' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"#### Broadcasting rules","metadata":{}},{"cell_type":"markdown","source":"### Einstein Summation","metadata":{}},{"cell_type":"code","source":"def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit -n 20 t5 = matmul(m1,m2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Forward and Backward Passes","metadata":{}},{"cell_type":"markdown","source":"### Defining and Initializing a Layer","metadata":{}},{"cell_type":"code","source":"def lin(x, w, b): return x @ w + b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\ny = torch.randn(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1 = torch.randn(100,50)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1)\nb2 = torch.zeros(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = lin(x, w1, b1)\nl1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1.mean(), l1.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\nfor i in range(50): x = x @ torch.randn(100,100)\nx[0:5,0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.01)\nx[0:5,0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\nfor i in range(50): x = x @ (torch.randn(100,100) * 0.1)\nx[0:5,0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\ny = torch.randn(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nw1 = torch.randn(100,50) / sqrt(100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) / sqrt(50)\nb2 = torch.zeros(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = lin(x, w1, b1)\nl1.mean(),l1.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relu(x): return x.clamp_min(0.)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l2 = relu(l1)\nl2.mean(),l2.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1))\nx[0:5,0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\nfor i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))\nx[0:5,0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(200, 100)\ny = torch.randn(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1 = torch.randn(100,50) * sqrt(2 / 100)\nb1 = torch.zeros(50)\nw2 = torch.randn(50,1) * sqrt(2 / 50)\nb2 = torch.zeros(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = lin(x, w1, b1)\nl2 = relu(l1)\nl2.mean(), l2.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    l3 = lin(l2, w2, b2)\n    return l3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = model(x)\nout.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = mse(out, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradients and the Backward Pass","metadata":{}},{"cell_type":"code","source":"def mse_grad(inp, targ): \n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp>0).float() * out.g","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = inp.t() @ out.g\n    b.g = out.g.sum(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sidebar: SymPy","metadata":{}},{"cell_type":"code","source":"from sympy import symbols,diff\nsx,sy = symbols('sx sy')\ndiff(sx**2, sx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### End sidebar","metadata":{}},{"cell_type":"code","source":"def forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu(l1)\n    out = l2 @ w2 + b2\n    # we don't actually need the loss in backward!\n    loss = mse(out, targ)\n    \n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Refactoring the Model","metadata":{}},{"cell_type":"code","source":"class Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp@self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mse():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n    \n    def backward(self):\n        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n        self.inp.g = 2.*x/self.targ.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(w1, b1, w2, b2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = model(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Going to PyTorch","metadata":{}},{"cell_type":"code","source":"class LayerFunction():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self):  raise Exception('not implemented')\n    def bwd(self):      raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Relu(LayerFunction):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Lin(LayerFunction):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = out.g.sum(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mse(LayerFunction):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): \n        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Function\n\nclass MyRelu(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.clamp_min(0.)\n        ctx.save_for_backward(i)\n        return result\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        i, = ctx.saved_tensors\n        return grad_output * (i>0).float()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LinearLayer(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n        self.bias = nn.Parameter(torch.zeros(n_out))\n    \n    def forward(self, x): return x @ self.weight.t() + self.bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin = LinearLayer(10,2)\np1,p2 = lin.parameters()\np1.shape,p2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(Module):\n    def __init__(self, n_in, nh, n_out):\n        self.layers = nn.Sequential(\n            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n        self.loss = mse\n        \n    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"## Questionnaire","metadata":{}},{"cell_type":"markdown","source":"1. Write the Python code to implement a single neuron.\n1. Write the Python code to implement ReLU.\n1. Write the Python code for a dense layer in terms of matrix multiplication.\n1. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n1. What is the \"hidden size\" of a layer?\n1. What does the `t` method do in PyTorch?\n1. Why is matrix multiplication written in plain Python very slow?\n1. In `matmul`, why is `ac==br`?\n1. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n1. What is \"elementwise arithmetic\"?\n1. Write the PyTorch code to test whether every element of `a` is greater than the corresponding element of `b`.\n1. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n1. What does this return, and why? `tensor([1,2]) + tensor([1])`\n1. What does this return, and why? `tensor([1,2]) + tensor([1,2,3])`\n1. How does elementwise arithmetic help us speed up `matmul`?\n1. What are the broadcasting rules?\n1. What is `expand_as`? Show an example of how it can be used to match the results of broadcasting.\n1. How does `unsqueeze` help us to solve certain broadcasting problems?\n1. How can we use indexing to do the same operation as `unsqueeze`?\n1. How do we show the actual contents of the memory used for a tensor?\n1. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n1. Do broadcasting and `expand_as` result in increased memory use? Why or why not?\n1. Implement `matmul` using Einstein summation.\n1. What does a repeated index letter represent on the left-hand side of einsum?\n1. What are the three rules of Einstein summation notation? Why?\n1. What are the forward pass and backward pass of a neural network?\n1. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n1. What is the downside of having activations with a standard deviation too far away from 1?\n1. How can weight initialization help avoid this problem?\n1. What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?\n1. Why do we sometimes have to use the `squeeze` method in loss functions?\n1. What does the argument to the `squeeze` method do? Why might it be important to include this argument, even though PyTorch does not require it?\n1. What is the \"chain rule\"? Show the equation in either of the two forms presented in this chapter.\n1. Show how to calculate the gradients of `mse(lin(l2, w2, b2), y)` using the chain rule.\n1. What is the gradient of ReLU? Show it in math or code. (You shouldn't need to commit this to memory—try to figure it using your knowledge of the shape of the function.)\n1. In what order do we need to call the `*_grad` functions in the backward pass? Why?\n1. What is `__call__`?\n1. What methods must we implement when writing a `torch.autograd.Function`?\n1. Write `nn.Linear` from scratch, and test it works.\n1. What is the difference between `nn.Module` and fastai's `Module`?","metadata":{}},{"cell_type":"markdown","source":"### Further Research","metadata":{}},{"cell_type":"markdown","source":"1. Implement ReLU as a `torch.autograd.Function` and train a model with it.\n1. If you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter.\n1. Learn about the `unfold` method in PyTorch, and use it along with matrix multiplication to implement your own 2D convolution function. Then train a CNN that uses it.\n1. Implement everything in this chapter using NumPy instead of PyTorch. ","metadata":{}},{"cell_type":"markdown","source":"## Xtra\n\n### myMatMul by hand\nTry diff dstructs and measure time","metadata":{}},{"cell_type":"code","source":"#myMatMul by hand \n#dstructs: lists\ndef myMatMul(m1, m2):\n    ar,ac = m1.shape\n    br,bc = m2.shape\n    #print((ar,ac), (br,bc))\n    assert ac==br\n\n    c=[]\n\n    for a_r in range(ar): #5 rows in m1\n        c_r=[]\n        \n        for b_c in range(bc): #10 cols in m2\n            c_val=[]\n\n            for b_r in range(br): #784 rows in m2\n                c_val.append(m1[a_r,b_r] * m2[b_r,b_c])\n\n            c_r.append(sum(c_val))\n\n        c.append(c_r)\n\n    #print (len(c))\n    return c\n\n#myMatMul1 by hand\n#dstructs: np array\ndef myMatMul1(m1, m2):\n    ar,ac = m1.shape\n    br,bc = m2.shape\n    assert ac==br\n\n    c=np.zeros((ar, bc), dtype=float)\n    \n    for a_r in range(ar): #5 rows in m1\n        #print(a_r)\n\n        for b_c in range(bc): #10 cols in m2\n            c_sum = 0\n            \n            for b_r in range(br): #784 rows in m2\n                c_val = m1[a_r,b_r] * m2[b_r,b_c]\n                c_sum += c_val\n                    \n            c[a_r, b_c] = c_sum\n\n    #print (len(c))\n    return c\n\n#myMatMul2 by hand - cleaner code but slower than myMatMul1\n#dstructs: np array\ndef myMatMul2(m1, m2):\n    ar,ac = m1.shape\n    br,bc = m2.shape\n    assert ac==br\n\n    c=np.zeros((ar, bc), dtype=float)\n    \n    for a_r in range(ar): #5 rows in m1\n        \n        for b_c in range(bc): #10 cols in m2\n            \n            for b_r in range(br): #784 rows in m2\n                c[a_r, b_c] += m1[a_r,b_r] * m2[b_r,b_c]\n                    \n    #print (len(c))\n    return c\n\n#myMatMul3 by hand - same as myMatMul2, only diff dstruct\n#dstructs: torch array\ndef myMatMul3(m1, m2):\n    ar,ac = m1.shape\n    br,bc = m2.shape\n    assert ac==br\n\n    c = torch.zeros(ar, bc)\n    \n    for a_r in range(ar): #5 rows in m1\n        \n        for b_c in range(bc): #10 cols in m2\n            \n            for b_r in range(br): #784 rows in m2\n                c[a_r, b_c] += m1[a_r,b_r] * m2[b_r,b_c]\n                    \n    #print (len(c))\n    return c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#$xtra my small 2x2 * 2x3 = 2x3 matrix\nm1 = tensor([[1, 2], [3, 4]])\nm2 = tensor([[1, 2, 3], [4, 5, 6]])\n\nm1, m2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time t1=matmul(m1, m2)\n%time m31=myMatMul1(m1,m2)\n%time m32=myMatMul2(m1,m2)\n%time m33=myMatMul3(m1,m2)\n%time torch_matmul=torch.matmul(m1,m2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can do faster with elementwise multiplcation - no loop k","metadata":{}},{"cell_type":"code","source":"#no loop k -> use elementwise multiplication instead\ndef myMatMul(a,b):\n    ar,ac = a.shape # n_rows * n_cols\n    br,bc = b.shape\n    assert ac==br\n    c = np.zeros((ar, bc), dtype=int32)\n    for i in range(ar): #2 rows in m1\n        for j in range(bc):\n            c[i,j]=(a[i]*b[:,j]).sum()\n        \n    return c\n\nmatmul(m1,m2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#no loop j -> use broadcasting instead\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = np.zeros((ar, bc), dtype=int)\n    for i in range(ar):\n        c[i] = (a[i] * b.T).sum(1)\n    return c\n\nmatmul(m1,m2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#no loop j -> use broadcasting instead\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = np.zeros((ar, bc), dtype=int)\n    print (a.dot(b))\n    #c = (a * b.T) #.sum(1)\n    #return c\n\nmatmul(m1,m2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shapes and Reshaping","metadata":{}},{"cell_type":"code","source":"#$xtra numpy shapes and reshaping\nc = np.array([10.,20,30]) #a vector with 3 elements\nc_reshaped = c.reshape((3,1))\nm = np.array([[1., 2, 3], [4,5,6], [7,8,9]])\nprint(c.shape, c_reshaped.shape, m.shape)\n\nc + m, c_reshaped + m\n#c, c_reshaped","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change the shape of our vector to make it a 3×1 matrix\nprint(m2[0].shape, m2[0].unsqueeze(-1).shape)\nm2[0], m2[0].unsqueeze(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(ar):\n        print(a[i]),\n        print(a[i].unsqueeze(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}